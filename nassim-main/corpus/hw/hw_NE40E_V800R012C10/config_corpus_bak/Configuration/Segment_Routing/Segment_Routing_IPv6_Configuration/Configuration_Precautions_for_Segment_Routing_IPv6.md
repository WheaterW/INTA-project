Configuration Precautions for Segment Routing IPv6
==================================================

Configuration_Precautions_for_Segment_Routing_IPv6

#### Feature Requirements

**Table 1** Feature requirements
| Feature Requirements | Series | Models |
| --- | --- | --- |
| The last-hop SID of a segment list cannot be any binding SID. Otherwise, traffic forwarding fails. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a trunk scheduling tree split scenario for P2P slices, after the HQoS scheduling mode is set to split, traffic on trunk member interfaces cannot be split based on weights. Resources are reserved on each trunk member interface based on the configured network slice bandwidth. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| BFD is used to detect the reliability of segment lists in an SRv6 TE Policy. If all segment lists in the primary candidate path fail, BFD triggers traffic to be switched to the backup candidate path. If all segment lists in the backup candidate path fail, BFD triggers the SRv6 TE Policy to go Down, and VPN FRR switching is triggered. Even if the SRv6 TE Policy has other candidate paths, candidate path selection is not performed again. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| 1. When the dynamic maximum reservable TE bandwidth is configured on a basic slice interface, the link bandwidth of the interface changes with the remaining bandwidth of the basic slice interface, but the TE bandwidth does not change with the remaining bandwidth. The control unit that uses the smaller value between the link bandwidth and the minimum TE bandwidth for path computation can properly perform path computation and optimization, but may fail to properly display TE bandwidth data.  2. If the dynamic maximum reservable TE bandwidth configured on a basic slice interface is less than 100%, TE services are expected to use only the remaining bandwidth of the basic slice interface. In fact, TE services may use excessive or even all the remaining bandwidth of the basic slice interface. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, BFD cannot detect the slice path specified using the ACL based on 5-tuple. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Slice ID allocation based on ACL matching 5-tuple is not supported in Layer 2 scenarios, including EVPN E-Line over SRv6, EVPN E-LAN over SRv6 and VLL over SRv6. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, the NE40E-M2E board without an eTM subcard preferentially schedules the bandwidth of slice SQs that are guaranteed in high-priority mode. The non-multiplexing mode is not supported. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In the scenario where the network slice bandwidth cannot be reused, if the remaining bandwidth is less than 50 Mbit/s after the interface/ channelized sub-interface bandwidth is deducted, the PIR bandwidth of the port takes effect by default as follows: MIN (port-shaping bandwidth, interface bandwidth, 50 Mbit/s) (basic protocol bandwidth reserved) | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If the encapsulation source-address command is not run, downstream devices may fail to forward SRv6 packets, causing an end-to-end service forwarding failure. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Compression cannot be performed for BSIDs, service chain SIDs, mirror SIDs, and End.OP SIDs | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| TE FRR cannot be deployed in VPN SID 16-bit compression scenarios. If a VPN SID is compressed to 16 bits, TE FRR cannot be deployed because the VPN SID and path SID are orchestrated together and cannot be offset to the egress. You are advised not to configure VPN SID compression. Configure compression for such SIDs only when required. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| VPN SID compression cannot be performed in EVPN VPLS BUM scenarios (End.DT2M). In EVPN VPLS and EVPN VPWS dual-homing scenarios, compression cannot be performed for bypass traffic. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| TI-LFA FRR and microloop avoidance support only 32-bit compression instead of 16-bit compression. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where 16-bit compressed SIDs are manually configured or delivered by the controller, if the involved End.X SID is not a complete SID, SRv6 TE Policy headend-based fault detection cannot correctly detect faults. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| The GIB ranges of node IDs of all devices (including non-compression devices) on the entire network must be the same. After a 16-bit compression locator is configured, the GIB ratio cannot be modified. It can be modified only after the locator is deleted. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| The path delivered by the controller must contain the End SID of the egress or the End.X SID of the penultimate hop. If this requirement is not met, traffic may fail to be forwarded in VPN SID compression. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where EVPN L3VPN over SRv6 and BGP L3VPN over MPLS interwork, DiffServ mode configuration for a VPN instance on the interworking node is different from that implemented in AC-side direct L3VPN access scenarios.  Traffic forwarding from an SRv6 tunnel to an MPLS tunnel: If the pipe mode is configured for a VPN instance, the EXP value in the MPLS label is encapsulated based on the priority of the outer IPv6 packet. If the short-pipe mode is configured for a VPN instance, the EXP value in the MPLS label is encapsulated based on the priority of the inner packet.  Traffic forwarding from an MPLS tunnel to an SRv6 tunnel: If the pipe mode is configured for a VPN instance, the Traffic Class field in IPv6 packets is encapsulated based on the EXP value in the MPLS label. If the short-pipe mode is configured for a VPN instance, the Traffic Class field in IPv6 packets is encapsulated based on the priority in the inner packet. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In L3VPN recursion to public network SRv6 tunnel scenarios, BFD for peer IP protection does not support the use of the original BGP next hop as the peer IP address. Instead, the locator address of the VPN SID should be used as the peer IP address. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In L3VPN recursion to public network SRv6 tunnel scenarios, the outbound interface cannot be a VLANIF, BDIF, or MPLS tunnel interface. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In L3VPN recursion to public network SRv6 tunnel scenarios, only packet information is sampled. Forwarding information such as the next hop and outbound interface cannot be sampled. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 egress nodes do not support in-depth load balancing. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 allows a packet to carry multiple SRHs, but only the first SRH is parsed. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 packets with SRHs cannot be fragmented when being forwarded within a tunnel. Each SRH supports a maximum of 10 SIDs.  You are advised to plan service configurations properly and set a small MTU for the packets to be steered into a tunnel. This avoids oversized packets in the tunnel. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 next hops do not support dynamic load balancing adjustment.  After dynamic load balancing is enabled, it does not take effect on the next hops of outbound interfaces on the SRv6 ingress and transit nodes. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Only the Next Header field of the IPv6 header can be an SRH. That is, the SRH can be located in the first IPv6 extension header field instead of being located after other extension headers. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a scenario where SRv6 TI-LFA is deployed between the ingress and a transit node, when traffic is switched to the backup path due to a fault of the primary path, the SRH information about the primary path and the SRH of the backup path are encapsulated into SRv6 packets. In this case, the packet length may exceed the interface MTU. However, the packets are sent to the CPU without being fragmented. Packet loss occurs during the primary/backup switchover.  You are advised to properly plan SRv6 MTUs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 egress protection scenarios, during the process of switching traffic to the backup path due to a fault of the primary path, SRH information about the primary and backup paths is added to SRv6 packets. In this case, the packet length may exceed the interface MTU. However, the packets are sent to the CPU without being fragmented. Packet loss occurs during the primary/backup switchover.  You are advised to properly plan SRv6 MTUs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 binding SID nodes do not support packet fragmentation.  Properly plan the SRv6 MTU. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 BE/SRv6 TE Policy TI-LFA protection scenarios, if the link MTU of the backup link is less than the SRv6 path MTU and the packet length is greater than the link MTU, traffic cannot be forwarded.  You are advised to properly plan the link MTU and SRv6 path MTU. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When SRv6 is deployed on the ingress and transit nodes, the packet length may exceed the IPv6 MTU of the interface after SRH information is encapsulated. During segment list configuration, if the first SID is a local End SID or binding SID, the local node does not fragment packets but sends them to the CPU. As a result, the packets cannot be forwarded.  Properly plan IPv6 MTUs for interfaces. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| After indirect next hop is configured, SRv6 does not support TE FRR if the explicit path label has only one outbound interface. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| End.DX4 and End.DX6 SIDs do not support VLANIF interfaces, low-speed interfaces, or VE interfaces. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If End.DX4 or End.DX6 SIDs are used and the outbound interface of services is a VBDIF interface, the services cannot access any VXLAN or VPLS network. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| A maximum of four levels of load balancing may be performed for services forwarded over SRv6 TE Policies:  1. VPN ECMP  2. Segment list-based ECMP  3. End SID-based load balancing  4. Load balancing among trunk member interfaces when the outbound interface is a trunk interface  A device supports a maximum of three levels of load balancing. If four levels of load balancing exist, load balancing among trunk member interfaces does not take effect. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a telco cloud scenario where BGP routes recurse to SRv6 remote cross routes, multiple VPN BGP peers are established for a PE, each peer address recurses to multiple SIDs for load balancing, and SIDs recurse to routes for trunk interface load balancing, load balancing may fail among trunk member interfaces. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| The reachability of the outermost SID in a segment list is imperceptible to the headend. As such, if multiple segment lists work in load balancing mode and the outermost SID in a segment list is unreachable, traffic forwarded through this segment list cannot be quickly switched to any other segment list. Traffic switching can be performed only after the controller detects the failure of the segment list, re-computes a path, and then deletes the faulty segment list. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| The reachability of the outermost SID in a segment list is imperceptible to the headend. As such, if an SRv6 TE Policy consists of only one segment list and the outermost SID in the segment list is unreachable, the SRv6 TE Policy does not go down, and traffic cannot be quickly switched to the VPN FRR backup path or best-effort path. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where binding SIDs are used, if the outermost SID of one SRv6 TE Policy is a binding SID that identifies another SRv6 TE Policy whose outermost SID is also a binding SID, traffic is always looped back on the headend until bandwidth resources are exhausted.  Ensure that configurations are correct and binding SIDs are not infinitely nested. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If the function to collect SRv6 TE Policy traffic statistics is enabled and a board whose traffic statistics are collected fails, the faulty board cannot report historical statistics. As a result, SRv6 TE Policy traffic statistics decrease sharply. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 inter-board VPN FRR switchback scenarios, the local device cannot guarantee the entry delivery sequence for upstream and downstream paths, which may cause inter-board packet loss. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where public network IPv4 traffic is redirected to an SRv6 TE Policy, if the redirection configuration or SRv6 TE Policy is deleted, or if the SRv6 TE Policy goes down, packet loss occurs when traffic is switched from the SRv6 TE Policy to an IP link.  Perform any of the following operations to prevent this problem:  1. Delete the traffic policy bound to the involved interface.  2. Delete the classifier behavior command configuration in the traffic policy view.  3. Delete the configured traffic matching rule. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In telco cloud scenarios, if the egress of an SR tunnel is configured to allocate a SID to each next hop and the peer end is an indirectly connected next hop, traffic cannot be properly forwarded by default.  If the peer end is an indirectly connected next hop, a route-policy must be configured on the egress node so that the route sent to the peer end carries the gateway IP address. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 BE and SRv6 TE Policy cannot be configured together with poison reverse in TI-LFA FRR or TE FRR scenarios. Otherwise, traffic may be interrupted. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC scenarios, only SFFs and SFs can be directly connected. In indirect connection scenarios, traffic may fail to be forwarded to SFs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Cache information configured in the SRv6 SFC static proxy scenario indicates the E2E path that needs to be re-encapsulated after the original packet is sent from the SF to the SFF. A maximum of 11 IPv6 addresses can be configured for the path, including one mandatory service SID and a maximum of 10 proxy SIDs. The path does not support BSID configuration or IPv6 address verification, which needs to be guaranteed by service configurations. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC static proxy scenarios, interfaces must be planned for exclusive use. Packets received through the inbound interface and packets whose sender (SF or not) cannot be identified re-enter the SFC. Routing protocols cannot run on SFC interfaces. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC static proxy scenarios where the Layer 3 mode is configured and only VBDIF interfaces can be used as inbound and outbound interfaces, if an EVC sub-interface added to a BD has services irrelevant to the SFC, irrelevant packets may be broadcast to SFs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC static proxy scenarios, the configured proxy SID supports dual-homing protection, but the backup SID that protects the proxy SID does not. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC static proxy scenarios where bypass protection is configured, two SFs support only unidirectional protection and do not support mutual protection. If mutual protection is configured, a loop may occur. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When an SRv6 SFC works in Layer 3 mode and dynamic ARP learning is implemented during link switchback between SFFs and SFs, a few packets are lost. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When an SRv6 SFC works in Layer 2 mode, BFD cannot be used to detect links between SFFs and SFs. If a fault occurs, service paths cannot be quickly switched. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where an SRv6 SFC works in Layer 3 mode, BFD is used to detect the link between SFFs and SFs, thereby implementing switching. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If SRv6 TE FRR is enabled in an SRv6 SFC scenario where the link between an SF and an SFF fails, traffic may bypass the SFF. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In an SRv6 SFC static proxy scenario, after a packet is returned from an SF, the SFF recalculates the HopLimit, Traffic Class, and FlowLabel fields when re-encapsulating the IPv6 packet header. In this case, the values may be different from those calculated by the ingress. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 SFC static proxy scenarios, ping and trace operations cannot be performed between SFFs and SFs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When an SRv6 SFC works in Layer 2 mode, traffic from SFFs to SFs does not support fragmentation.  Properly plan the MTU. If fragmentation is required, fragment packets when the packets enter the SFC on the SC. Do not fragment packets on transit nodes. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| For the consistency of hash results based on which load balancing is performed for upstream and downstream traffic on an SRv6 SFC, pay attention to the following points:  1. If different hardware is used, hash results may be inconsistent.  2. The random algorithm is not supported due to the use of a random polynomial. If the random algorithm is used, hash results may be inconsistent.  3. Upstream and downstream traffic must have the same hash factors (5-tuple or 3-tuple) after the factors are sorted based on certain rules. Otherwise, hash results may be inconsistent.  4. If a Huawei device is connected to a non-Huawei device, different load balancing algorithms may be used, causing hash results to be inconsistent.  5. If IP addresses are not used as hash factors, different hash factors are configured on the SF and SFF, or only the source or destination IP address is used as the hash factor, hash results may be inconsistent.  6. In fault protection scenarios, hash results may be inconsistent.  7. If the number of links used for load balancing of upstream traffic is different from that used for load balancing of downstream traffic, hash results may be inconsistent.  8. The scenario where value-added services are deployed on a centralized board is not supported.  9. Fragmented and non-fragmented packets of the same flow are transmitted through different paths. For fragmented packets, the first fragment and subsequent fragments are transmitted through the same path.  10. Only IPv4 packets can be forwarded. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where an SRv6 TE Policy consists of multiple segment lists that work in load balancing mode, if more than seven segment lists fail concurrently, fast traffic switching is not supported.  Suggestions:  1. Temporarily isolate the failed segment lists after their failures are detected. (This is the default operation that will be performed in this case.)  2. Deploy VPN FRR. In this way, VPN FRR switching is triggered if more than seven segment lists fail. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where an SRv6 TE Policy consists of multiple segment lists that work in load balancing mode and SBFD for segment list is deployed, SBFD return packets are forwarded over IP links, and the SBFD return packets of the segment lists share the same path. If the return path fails, all SBFD for segment list sessions go down, resulting in service switching.  You are advised to deploy VPN FRR so that VPN FRR switching can be triggered if traffic is falsely switched. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE Policy multi-level load balancing (service-based load balancing + SRv6 TE flow group-based load balancing + segment list-based load balancing + load balancing based on the first SID of a segment list + trunk-based load balancing) scenarios where more than three levels of load balancing are performed: If the number of channels involved in level-1 load balancing and that involved in level-4 load balancing are integer multiples of each other and the hash factors used for the two levels of load balancing are the same, level-4 load balancing fails. This limitation also applies to level-2 and level-5 load balancing.  In this case, you can run the load-balance hash-arithmetic command to change the hash algorithm to the XOR algorithm in order to add one level of load balancing. For details, see the description of the load-balance hash-arithmetic command in the product documentation. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In L3VPN/EVPN L3VPN load balancing over SRv6 TE flow group scenarios where SBFD for segment list is deployed, if the egress PE fails, fast service switching cannot be performed. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Network slices use the HBH or HBH2 extension header to carry slice IDs. The entire network must be upgraded to V800R013C00 or later. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, the slice IDs on a main interface must be unique. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In the network slicing scenario, the slice IDs in the same FlexE group (same ID) must be unique. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, slice interfaces and base interfaces cannot cross physical interfaces, Eth-Trunk interfaces, or FlexE group IDs. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, sub-interfaces bound to slice IDs support only the vlan-type dot1q type. Sub-interfaces with VLAN segments configured are not supported. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In the network slicing scenario, only one MP2MP network slice ID can be configured for a slice interface. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, if an SRv6 TE Policy uses loose explicit paths that work in load-balancing mode, the same slice must be configured on each outbound interface. Configuring slices only for some links is not allowed. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, the hash combination of default slices on base interfaces is used for load balancing. The hash algorithm is unaware of the slice bandwidth and may result in uneven hash distributions. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a network slicing scenario where a base interface and a slicing interface belong to different FlexE main interfaces, if the base interface is faulty but the slicing interface is not, route convergence is required, and the fault recovery time may exceed 50 ms; if the slice interface is faulty but the base interface is not, services are switched to the base interface after the slicing interface algorithm table is deleted by the software, and the fault recovery time may exceed 50 ms. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a network slicing P2P private line scenario, the port-queue cs6 and port share-shaping cs6 commands are mutually exclusive on the same physical interface of the NE40E-M2E board.  In a network slicing P2P leased line scenario, the port-queue ef and port share-shaping ef commands are mutually exclusive on the same trunk interface of the NE40E-M2E board, and the port-queue cs6 and port share-shaping cs6 commands are mutually exclusive, the port-queue cs7 and port share-shaping cs7 commands are mutually exclusive. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In network slicing scenarios, if the sum of the rate limits for service SQs (including QoS profile SQs and flow SQs) and P2P SQs exceeds the interface bandwidth (or rate configured using the port-shaping command), the bandwidth and delay of P2P SQs cannot be ensured due to interface backpressure. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If traffic is steered into SRv6 TE Policies based on service class values, behavior aggregate classification and ACL-based multi-field classification can be configured, but mapping CAR values to service class values is not supported. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 BE network slicing scenarios where VPN FRR or load balancing is configured, if the colors carried by routes are different, the color of the optimal route is selected and packets are sent to the corresponding slice based on the color. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 over GRE, if the length of an encapsulated packet exceeds the SRv6 or GRE tunnel MTU, the packet will be discarded. You are advised to properly plan the path MTU to prevent fragmentation. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a scenario where public network service traffic is redirected to an SRv6 TE Policy through multi-field classification or FlowSpec, if no SID is specified and the SRv6 TE Policy does not have the USD capability, the traffic is directly forwarded through IP instead of entering the SRv6 TE Policy. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| For DSCP-based traffic steering in L3VPN over SRv6 TE flow group scenarios, services cannot be transmitted over both native-IP and Flex-Algo paths. If the locator to which a service VPN SID belongs is configured to be added to a Flex-Algo, traffic may be interrupted.  Avoid the preceding configuration. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE Policy shortcut to SRv6 TE flow group scenarios where traffic is steered into an SRv6 TE Policy based on the DSCP values of packets, encapsulated packets need to be looped back, which causes the forwarding performance to be reduced by half and the DSCP-based traffic steering result to be inaccurate.  You are advised to properly plan configurations to prevent an SRv6 TE Policy and an SRv6 TE flow group from sharing the same headend. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE flow group shortcut scenarios where an SRv6 TE flow group and a physical interface work in load balancing mode, if the physical interface has an FRR backup link, only the primary link instead of the backup link is used in the forwarding plane. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In scenarios where public network IPv4 traffic is redirected to an SRv6 TE Policy through multi-field classification or FlowSpec, if no SID is specified, the last SID of the SRv6 TE Policy is an End.X SID that supports the USD flavor, and network slicing is configured, network slicing does not take effect for the last SID. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a shortcut-based SRv6 BE/SRv6 TE Policy over SRv6 TE Policy scenario where network slicing is configured for the original SRv6 BE/SRv6 TE Policy but the target SRv6 TE Policy uses the Encaps mode and is not configured with network slicing, if the slice ID configured for the original SRv6 BE/SRv6 TE Policy is the same as that of the slice interface configured on the local device, traffic may be unexpectedly forwarded through the slice interface corresponding to the slice ID configured for the original SRv6 BE/SRv6 TE Policy. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If the basic-slice configuration is not performed on the slice interface, the IPv6 MTU of the slice interface is not inherited from the basic interface when packets are forwarded from the slice interface. The IPv6 MTU depends on the configuration of the current slice interface. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If no basic slice is configured on the slice interface or the configured basic slice ID is different from the slice ID of the base interface, after the traffic is forwarded through the slice interface, if the encapsulated destination MAC address does not match the MAC address of the peer receive interface, the packet is discarded on the peer end. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 BE network slicing scenarios, if routes carry different colors in VPN FRR and load balancing scenarios, the color of the optimal route is selected and the corresponding slice is added based on the color. You are advised to configure routes to carry the same color in VPN FRR and load balancing scenarios. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| IPv6 Segment Routing: End SIDs must be globally unique. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| A Layer 3 link between two nodes passes through a Layer 2 switch, and a static End.X SID is configured for the Layer 3 link. When the path of an SRv6 TE Policy contains the static End.X SID, the local node cannot detect the link fault between the switch and peer node. As a result, packet loss occurs continuously in the SRv6 TE Policy. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| Pay attention to the following points in SRv6 SFC static proxy scenarios:  1. In scenarios where an SF is dual-homed to SFFs, if the peer-SID configured on SFF1 is an End SID on SFF2, only an IPv6 address validity check is performed. The configuration performed by users determines whether the SID is of the End type. In addition, only global configuration is available in current scenarios, and there are requirements for controller adaptation.  2. In scenarios where an SF is dual-homed to SFFs, the backup SID is the protection SID of a proxy SID and is not on the protection path. SFF1 and SFF2 that protect each other must be configured with the same backup SID.  3. In bypass protection scenarios, unidirectional protection instead of mutual protection is implemented for SF1 and SF2. If mutual protection is configured, SF1 and SF2 provide bypass protection for each other, which may lead to a loop.  4. In transparent SF forwarding, if EVC sub-interfaces are used as the out-interface and in-interface, the out-interface is an SFF-to-SF interface and the in-interface is an SF-to-SFF interface.  5. In transparent SF forwarding, EVC sub-interfaces support only basic flow encapsulation instead of default encapsulation. In addition, the encapsulation configuration of the SFC must match those of the involved interface. Otherwise, traffic will be interrupted. Ensure that the configuration is properly planned. In transparent SF forwarding, EVC sub-interfaces do not support traffic rewrite actions.  6. Ping and trace operations cannot be performed between an SFF and an SF.  If global configurations are not correctly planned, traffic may be interrupted. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| 1. Fault detection only checks whether the SID exists, but does not check the connectivity of the path.  2. Fault detection can only verify the SID status in a single IGP domain, but cannot verify the SID status in cross-domain, cross-process, cross-area, or cross-level scenarios.  3. Fault Detection If the SID stack contains SIDs that cannot be flooded, such as binding SIDs and BGP EPE SIDs, the verification fails.  4. The fault detection function requires that SRV6 be enabled for IS-IS on the ingress. Otherwise, topology information cannot be collected, causing a verification failure.  5. When the fault detection function detects SIDs flooded by OSPFv3 in the SID stack, the verification fails. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In VRP-V800R019C00 and earlier versions, VPN data forwarding over SRv6 BE paths is controlled only by the interface MTU. In VRP-V800R019C10 and later versions, VPN data forwarding over SRv6 BE paths is controlled by the smaller value between the interface MTU and the path MTU configured in the Segment Routing IPv6 view. If the source and target versions cross VRP-V800R019C10 and the interface MTU is greater than 1500 in the source version, VPN data flows are controlled based on 1500 bytes (the default SRv6 path MTU) in the target version. This ensures that the length of the packet encapsulated with the SRv6 BE IPv6 header does not exceed 1500 bytes. (If packets in the VPN data flow are IPv4 packets, the packets are fragmented and encapsulated with the SRv6 BE IPv6 header. If the packets in the VPN data flow are IPv6 packets, the packets are discarded and ICMPv6 Packet Too BIG packets are returned.)  In a scenario where the value of path-mtu decreases to be less than the interface MTU, if the packets in the VPN data flow are IPv4 packets, more packet fragments will be generated; if the packets in the VPN data flow are IPv6 packets, more packets will be discarded. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When Segment Routing IPv6 is deployed on a network, IPv6 packet headers occupy a large amount of payload space. Therefore, MTU planning must be carefully considered and some space must be reserved for the SRH address stack. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In a scenario where a device is restarted, if a BFD session or peer is in the Admin Down state, the SRv6 TE Policy status is not affected. However, if the BFD session is renegotiated and goes Down, the SRv6 TE Policy also goes Down. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| IS-IS TI-LFA FRR cross-level protection scenario:  1. Cross-level protection is supported only in an IS-IS process. Cross-process or cross-protocol protection is not supported.  2. Command-based unified control is supported only in an IS-IS process.  3. Only SRv6 scenarios are supported.  4. Level-2 protection paths can be provided for IS-IS Level-1 routes, but Level-1 protection paths cannot be provided for IS-IS Level-2 routes.  5. It is recommended that this function be used only in open-loop access ring scenarios. In closed-loop scenarios, if cross-level protection is forcibly used, fast switching may not be achieved. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When public network traffic enters an SRv6 TE Policy, no public SID is available, and SL[0] is used for compression orchestration, a forwarding problem occurs in a TE FRR scenario.  Workaround: When compression locator and TE FRR are deployed, public SIDs must be configured for public network traffic to enter SRv6 TE Policies. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| After a non-compression locator is upgraded to a compression locator, dynamic SIDs may change, adversely affecting traffic forwarding. Traffic forwarding is restored after dynamic SIDs are regenerated. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| SRv6 path MTU configurations do not take effect in the following scenarios: EVPN VPWS over SRv6 BE and EVPN VPLS over SRv6 BE | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| The BSID pre-occupation function has the following restrictions:  1. Before changing the range of pre-occupied BSIDs to a smaller value or deleting a pre-occupied BSID, release the occupied BSIDs. Otherwise, the configuration fails and an error is reported.  2. The current BSID is occupied by the controller. Therefore, you need to release the BSID through the controller. To forcibly release all occupied BSIDs through the device, you can disable the corresponding service capability.  3. After the configuration is modified, the BSID range pre-occupied by the previous configuration may be occupied by other services. As a result, the configuration rollback may fail.  4. The pre-occupied BSID starts from the maximum value of the non-compressed dynamic segment. If the maximum value of the non-compressed dynamic segment is occupied, no BSID can be pre-occupied.  5. After the locator compression attribute is modified, the range of the pre-occupied SID resource pool changes. As a result, dynamic SIDs in the pre-occupied range are released, affecting traffic forwarding. After the compression attribute is modified, the SID in the old range fails to be occupied and services cannot be restored. In this case, you need to specify the SID in the new range. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In the scenario where BFD return packets are forwarded over a tunnel:  1. During the switching between different types of BFD sessions, the BFD status of the segment list remains unchanged before the new BFD session state is returned. If the original BFD session is up and the involved link goes down during the process of deleting the original BFD session and adding a new one, the segment list remains up until the new BFD session state becomes down. (If the link does not go up, the BFD session goes down after 5 minutes, during which continuous packet loss occurs.)  2. In a scenario where the BSID of a segment list is updated, if the forwarding entry corresponding to the new BSID has not been delivered successfully, BFD may go down.  3. In a path change scenario, if the new path does not go Up due to BFD negotiation for a long time and the original path goes Down due to the timeout of delayed deletion of the corresponding return tunnel, packet loss may occur. To solve this problem, change the deletion delay of the SRv6 TE Policy. Ensure that the original path is not deleted before the BFD detection result of the new path is determined. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| If the BSID configured for an SRv6 TE Policy is one of the SIDs in another policy's segment list and the former policy adopts the Encaps encapsulation mode, the restrictions are as follows:  1, The PHP function is not supported.  2, If an SRv6 TE Policy has only one SID with the USD flavor, reduced encapsulation is not supported for the SRv6 TE Policy. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| When the binding SID of an SRv6 TE Policy uses the Encaps mode and network slicing is deployed, TE FRR cannot be implemented across IPv6 headers. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE Policy scenarios, if service traffic is redirected to multiple SRv6 TE Policies through MF classification, per-packet load balancing cannot be performed between SRv6 TE Policies. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TI-LFA and microloop avoidance scenarios, loopback processing is required when Encaps encapsulation is performed for SRv6 BE traffic. As a result, the forwarding bandwidth decreases by half. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE Policy scenarios where traffic is steered based on TE class values, outbound interface information cannot be collected through NetStream. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 TE Policy-based traffic steering scenarios, TE Class-based traffic steering does not support QPPB. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 network slicing scenarios, a P2P slice is configured for a trunk interface, and scheduling tree split is configured. The reserved bandwidth requested by the P2P slice is replicated based on member interfaces. After a P2P slice is configured for a trunk channelized sub-interface and trunk scheduling tree split is configured, the trunk member interfaces may fail to ensure the bandwidth reserved for slices. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |
| In SRv6 network slicing scenarios, if both the network-slice <slice-id> { data-plane | flex-channel <flex-channel-value> } and bas commands are run on an interface, SRv6 network slice forwarding and rate limiting cannot be ensured. Therefore, proper planning is required to prevent both commands from being configured. | NE40E-M2 | NE40E-M2E/NE40E-M2F/NE40E-M2H/NE40E-M2K/NE40E-M2K-B |